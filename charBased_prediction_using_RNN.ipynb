{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kdwhu6YImntQ",
        "outputId": "92995d54-6440-4913-d8fd-90dd068eee5c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl-12cdkDNBt",
        "outputId": "4c47b240-2250-4e50-b500-99dfd433c858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=2118f50d72e9f881c6a8c0f09b3ddc0908df69f296511b919b5de3bef6a3720b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECdfmx5bDNvd",
        "outputId": "f8090b19-c43b-4b81-bbd1-45606d6e0d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          4500      \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 150)               37650     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 45)                6795      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 48945 (191.19 KB)\n",
            "Trainable params: 48945 (191.19 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "650/650 [==============================] - 41s 59ms/step - loss: 2.5226 - accuracy: 0.2738\n",
            "Epoch 2/30\n",
            "650/650 [==============================] - 30s 46ms/step - loss: 2.1835 - accuracy: 0.3502\n",
            "Epoch 3/30\n",
            "650/650 [==============================] - 30s 46ms/step - loss: 2.0208 - accuracy: 0.3972\n",
            "Epoch 4/30\n",
            "650/650 [==============================] - 30s 46ms/step - loss: 1.8890 - accuracy: 0.4396\n",
            "Epoch 5/30\n",
            "650/650 [==============================] - 30s 47ms/step - loss: 1.7846 - accuracy: 0.4709\n",
            "Epoch 6/30\n",
            "650/650 [==============================] - 38s 59ms/step - loss: 1.6982 - accuracy: 0.4947\n",
            "Epoch 7/30\n",
            "650/650 [==============================] - 30s 46ms/step - loss: 1.6230 - accuracy: 0.5180\n",
            "Epoch 8/30\n",
            "650/650 [==============================] - 29s 45ms/step - loss: 1.5593 - accuracy: 0.5355\n",
            "Epoch 9/30\n",
            "650/650 [==============================] - 30s 45ms/step - loss: 1.4999 - accuracy: 0.5510\n",
            "Epoch 10/30\n",
            "650/650 [==============================] - 31s 48ms/step - loss: 1.4466 - accuracy: 0.5683\n",
            "Epoch 11/30\n",
            "650/650 [==============================] - 30s 45ms/step - loss: 1.3953 - accuracy: 0.5859\n",
            "Epoch 12/30\n",
            "650/650 [==============================] - 30s 46ms/step - loss: 1.3554 - accuracy: 0.5930\n",
            "Epoch 13/30\n",
            "650/650 [==============================] - 30s 46ms/step - loss: 1.3127 - accuracy: 0.6046\n",
            "Epoch 14/30\n",
            "650/650 [==============================] - 33s 51ms/step - loss: 1.2747 - accuracy: 0.6151\n",
            "Epoch 15/30\n",
            "650/650 [==============================] - 32s 50ms/step - loss: 1.2368 - accuracy: 0.6253\n",
            "Epoch 16/30\n",
            "650/650 [==============================] - 35s 54ms/step - loss: 1.2070 - accuracy: 0.6306\n",
            "Epoch 17/30\n",
            "650/650 [==============================] - 34s 52ms/step - loss: 1.1731 - accuracy: 0.6476\n",
            "Epoch 18/30\n",
            "650/650 [==============================] - 31s 48ms/step - loss: 1.1431 - accuracy: 0.6545\n",
            "Epoch 19/30\n",
            "650/650 [==============================] - 32s 49ms/step - loss: 1.1169 - accuracy: 0.6597\n",
            "Epoch 20/30\n",
            "650/650 [==============================] - 33s 51ms/step - loss: 1.0916 - accuracy: 0.6691\n",
            "Epoch 21/30\n",
            "650/650 [==============================] - 32s 49ms/step - loss: 1.0663 - accuracy: 0.6743\n",
            "Epoch 22/30\n",
            "650/650 [==============================] - 31s 48ms/step - loss: 1.0486 - accuracy: 0.6786\n",
            "Epoch 23/30\n",
            "650/650 [==============================] - 33s 51ms/step - loss: 1.0275 - accuracy: 0.6863\n",
            "Epoch 24/30\n",
            "650/650 [==============================] - 32s 49ms/step - loss: 1.0078 - accuracy: 0.6933\n",
            "Epoch 25/30\n",
            "650/650 [==============================] - 33s 51ms/step - loss: 0.9901 - accuracy: 0.6968\n",
            "Epoch 26/30\n",
            "650/650 [==============================] - 33s 51ms/step - loss: 0.9738 - accuracy: 0.7016\n",
            "Epoch 27/30\n",
            "650/650 [==============================] - 33s 50ms/step - loss: 0.9604 - accuracy: 0.7054\n",
            "Epoch 28/30\n",
            "650/650 [==============================] - 34s 52ms/step - loss: 0.9451 - accuracy: 0.7099\n",
            "Epoch 29/30\n",
            "650/650 [==============================] - 33s 51ms/step - loss: 0.9297 - accuracy: 0.7126\n",
            "Epoch 30/30\n",
            "650/650 [==============================] - 33s 50ms/step - loss: 0.9172 - accuracy: 0.7171\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c51e4dd1cf0>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "import wikipedia\n",
        "\n",
        "# preprocess text at the character level\n",
        "def preprocess_text_char(text):\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = re.sub(r'http[s]?://\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "\n",
        "    # Keep only alphabetic characters and spaces\n",
        "    filtered_text = [char for char in text if char.isalpha() or char.isspace()]\n",
        "\n",
        "    return ''.join(filtered_text)  # Join characters back into a string\n",
        "\n",
        "topic = \"Visual Arts\"\n",
        "wikipedia.set_lang(\"en\")  \n",
        "try:\n",
        "    wikipedia_page = wikipedia.page(topic)\n",
        "    wikipedia_text = wikipedia_page.content\n",
        "except wikipedia.exceptions.PageError:\n",
        "    print(\"Page not found. Please try another topic.\")\n",
        "    exit()\n",
        "\n",
        "preprocessed_text = preprocess_text_char(wikipedia_text)\n",
        "\n",
        "seq_length = 100  # Length of input sequences\n",
        "sequences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(len(preprocessed_text) - seq_length):\n",
        "    sequences.append(preprocessed_text[i:i + seq_length])\n",
        "    next_chars.append(preprocessed_text[i + seq_length])\n",
        "\n",
        "# Convert sequences to numeric data\n",
        "char_tokenizer = Tokenizer(char_level=True)\n",
        "char_tokenizer.fit_on_texts(sequences)\n",
        "\n",
        "# Convert characters to sequences of integers for sequences\n",
        "sequences = char_tokenizer.texts_to_sequences(sequences)\n",
        "sequences = np.array(sequences)\n",
        "\n",
        "# Total vocabulary size (number of unique characters)\n",
        "vocab_size = len(char_tokenizer.word_index) + 1\n",
        "\n",
        "model_char = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=100, input_length=seq_length),\n",
        "    SimpleRNN(units=150),\n",
        "    Dense(units=vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model_char.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_char.summary()\n",
        "\n",
        "sequences = sequences.astype(np.float32)  # Ensure sequences are float32\n",
        "next_sequences = np.array([char_tokenizer.texts_to_sequences([char])[0][0] for char in next_chars])  # Convert next_chars to sequences\n",
        "next_sequences = next_sequences.astype(np.int32)  # Ensure next_sequences are int32\n",
        "\n",
        "model_char.fit(sequences, next_sequences, epochs=30, verbose=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_fo-uWFcCbd",
        "outputId": "406c00e2-2a03-4aca-d9e7-b7157b1456f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Next character prediction: painter or in artists and plastic arts and conceptual and painters the earloch considered by materials a\n"
          ]
        }
      ],
      "source": [
        "def generate_text(seed_text, max_length=500):\n",
        "    generated_text = seed_text\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Tokenize the current generated text\n",
        "        token_list = char_tokenizer.texts_to_sequences([generated_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=seq_length, padding='pre')\n",
        "\n",
        "        # Predict the next character\n",
        "        predicted_char_idx = np.argmax(model_char.predict(token_list), axis=-1)\n",
        "\n",
        "        # Convert the predicted character index to a list\n",
        "        predicted_char_idx_list = predicted_char_idx.tolist()\n",
        "\n",
        "        # Get the predicted character from the tokenizer\n",
        "        predicted_char = char_tokenizer.sequences_to_texts([predicted_char_idx_list])[0]\n",
        "\n",
        "        # Append the predicted character to the generated text\n",
        "        generated_text += predicted_char\n",
        "\n",
        "        # Break if the predicted character is a newline or end-of-sentence marker\n",
        "        if predicted_char == '\\n':\n",
        "            break\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Testing character-based prediction\n",
        "test_char = \"pain\"\n",
        "predicted_next_char = generate_text(test_char, max_length=100)\n",
        "print(\"Next character prediction:\", predicted_next_char)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
